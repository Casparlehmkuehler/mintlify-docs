---
title: "Machine Types"
description: "Available computational resources for executing your workloads"
icon: "server"
---

## Overview

Lyceum Cloud provides various machine types optimized for different computational workloads. The platform offers both CPU and GPU configurations to match your specific requirements.

## Execution Types

When starting an execution through the API, you can specify the `execution_type` parameter:

```json
{
  "code": "your_python_code",
  "execution_type": "cpu"  // or "gpu", "auto"
}
```

### Available Options
- **cpu**: Standard CPU execution environment
- **gpu**: GPU-accelerated execution
- **auto**: Automatic selection based on your code

## Automatic Resource Selection

The platform can automatically select appropriate resources by analyzing your code:

```python
# The platform detects ML frameworks and selects GPU automatically
import torch
import tensorflow as tf

# Your deep learning code here
model = torch.nn.Sequential(...)
```

When using `execution_type: "auto"`, the system will:
- Detect imported packages and frameworks
- Estimate resource requirements
- Select the most appropriate machine type
- Optimize for cost-effectiveness

## Resource Monitoring

Monitor resource usage during execution to optimize your selection:

```python
import psutil

# Check CPU usage
cpu_percent = psutil.cpu_percent(interval=1)
print(f"CPU Usage: {cpu_percent}%")

# Check memory usage
memory = psutil.virtual_memory()
print(f"Memory Usage: {memory.percent}%")

# Check if GPU is available
try:
    import torch
    if torch.cuda.is_available():
        print(f"GPU Available: {torch.cuda.get_device_name()}")
        print(f"GPU Memory: {torch.cuda.mem_get_info()}")
except ImportError:
    print("PyTorch not installed")
```

## Choosing the Right Type

### Use CPU for:
- Data processing and analysis with pandas/numpy
- Web scraping and API calls
- File processing and transformations
- Statistical computing
- General Python scripts

### Use GPU for:
- Deep learning model training
- Computer vision tasks
- Natural language processing with transformers
- Parallel computing with CUDA
- Large-scale matrix operations

## Timeout Settings

Configure execution timeouts (1-600 seconds) to prevent runaway processes:

```json
{
  "code": "your_code",
  "timeout": 300,  // 5 minutes
  "execution_type": "cpu"
}
```

## Environment Variables

Access execution environment information:

```python
import os

# Check execution type
execution_type = os.environ.get('LYCEUM_EXECUTION_TYPE', 'unknown')
print(f"Running on: {execution_type}")

# Check available resources
if os.environ.get('CUDA_VISIBLE_DEVICES'):
    print("GPU execution environment detected")
```

## Storage Integration

All machine types have access to your cloud storage:

```python
# Files are available at the storage path
storage_path = os.environ.get('LYCEUM_STORAGE_PATH', '/lyceum/storage/')

# Access your uploaded files
import pandas as pd
df = pd.read_csv("{storage_path}")

# Save results
df.to_csv("{storage_path}")
```

<Note>
For current pricing and detailed specifications, check the dashboard or contact support. Machine type availability may vary based on demand.
</Note>