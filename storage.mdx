---
title: "Storage"
description: "S3-compatible storage integrated with Lyceum Cloud for managing data files, models, and execution outputs"
icon: "database"
---

<Info>
  Lyceum provides integrated S3 storage that automatically mounts to your execution environments. Upload once, access everywhere.
</Info>

## Overview

Lyceum Cloud provides S3-compatible storage that seamlessly integ rates with your computational workloads. Upload data files, scripts, and dependencies, then access them from any execution environment.

<CardGroup cols={3}>
  <Card title="Persistent Storage" icon="hard-drive">
    Files persist across executions with automatic backup
  </Card>
  <Card title="S3 Compatible" icon="aws">
    Full S3 API compatibility with familiar tools
  </Card>
  <Card title="Auto-mounting" icon="link">
    Files automatically available in all environments
  </Card>
</CardGroup>

## Upload Files

<Tabs>
  <Tab title="VS Code" icon="code">
    ### Quick Upload Methods
    
    <Steps>
      <Step title="Right-click Upload">
        Right-click any file or folder → "Upload to Lyceum Cloud"
      </Step>
      <Step title="Keyboard Shortcut">
        Press `Cmd+Shift+U` (Mac) or `Ctrl+Shift+U` (Windows/Linux)
      </Step>
      <Step title="Cloud Files Panel">
        Drag files to the Lyceum Cloud Files panel in Explorer
      </Step>
    </Steps>

    <Tip>
      The VS Code extension provides the fastest way to upload files during development
    </Tip>
  </Tab>

  <Tab title="Web Dashboard" icon="browser">
    ### Dashboard Upload
    
    <Steps>
      <Step title="Navigate to Files">
        Go to [dashboard.lyceum.technology](https://dashboard.lyceum.technology) → Files
      </Step>
      <Step title="Select Files">
        Click "Upload Files" or drag and drop your files
      </Step>
      <Step title="Monitor Progress">
        Track upload progress in real-time
      </Step>
    </Steps>
  </Tab>

  <Tab title="API" icon="code">
    ### Programmatic Upload

    ```bash
    curl -X POST https://api.lyceum.technology/api/v2/external/storage/upload \
      -H "Authorization: Bearer <token>" \
      -F "file=@/path/to/your/file.txt" \
      -F "key=data/file.txt"
    ```

    <CodeGroup>
      ```python Python
      import requests

      with open('data.csv', 'rb') as f:
          response = requests.post(
              'https://api.lyceum.technology/api/v2/external/storage/upload',
              headers={'Authorization': f'Bearer {token}'},
              files={'file': f},
              data={'key': 'datasets/data.csv'}
          )
      ```

      ```javascript JavaScript
      const formData = new FormData();
      formData.append('file', fileInput.files[0]);
      formData.append('key', 'datasets/data.csv');

      await fetch('https://api.lyceum.technology/api/v2/external/storage/upload', {
        method: 'POST',
        headers: { 'Authorization': `Bearer ${token}` },
        body: formData
      });
      ```
    </CodeGroup>
  </Tab>
</Tabs>

## Access Files in Code

Files are automatically mounted in your execution environment at `/lyceum/storage/`:

<CodeGroup>
  ```python Load Data
  import pandas as pd
  
  # Files automatically available at /lyceum/storage/
  df = pd.read_csv('/lyceum/storage/datasets/data.csv')
  
  # Process your data
  results = df.describe()
  
  # Save outputs back to storage
  results.to_csv('/lyceum/storage/outputs/results.csv')
  ```

  ```python Environment Variables
  import os
  
  # Access storage paths via environment variables
  storage_path = os.environ.get('LYCEUM_STORAGE_PATH', '/lyceum/storage/')
  bucket_name = os.environ.get('LYCEUM_BUCKET_NAME')
  s3_endpoint = os.environ.get('LYCEUM_S3_ENDPOINT')
  ```

  ```python List Files
  import os
  
  # List all files in storage
  for root, dirs, files in os.walk('/lyceum/storage/'):
      for file in files:
          print(os.path.join(root, file))
  ```
</CodeGroup>

## File Management

<AccordionGroup>
  <Accordion title="List Files" icon="list">
    ### API Request
    ```bash
    curl -X GET "https://api.lyceum.technology/api/v2/external/storage/list-files" \
      -H "Authorization: Bearer <token>" \
      -G -d "prefix=data/" -d "max_files=100"
    ```

    ### Response
    ```json
    {
      "success": true,
      "data": [
        {
          "key": "data/dataset.csv",
          "size": 1024000,
          "last_modified": "2024-01-15T10:30:00Z",
          "etag": "\"9bb58f26192e4ba00f01e2e7b136bbd8\""
        }
      ]
    }
    ```
  </Accordion>

  <Accordion title="Download Files" icon="download">
    ### VS Code Extension
    Click the download icon next to any file in the Cloud Files panel

    ### API Request
    ```bash
    curl -X GET "https://api.lyceum.technology/api/v2/external/storage/download/{file_key}" \
      -H "Authorization: Bearer <token>" \
      -o downloaded_file.txt
    ```
  </Accordion>

  <Accordion title="Delete Files" icon="trash">
    ### Single File
    ```bash
    curl -X DELETE "https://api.lyceum.technology/api/v2/external/storage/delete/{file_key}" \
      -H "Authorization: Bearer <token>"
    ```

    ### Entire Folder
    ```bash
    curl -X DELETE "https://api.lyceum.technology/api/v2/external/storage/delete-folder/{folder_prefix}" \
      -H "Authorization: Bearer <token>"
    ```
  </Accordion>
</AccordionGroup>

## S3 Client Access

Generate temporary S3 credentials for direct access using standard S3 tools:

<Tabs>
  <Tab title="Get Credentials" icon="key">
    ```bash
    curl -X POST https://api.lyceum.technology/api/v2/external/storage/credentials \
      -H "Authorization: Bearer <token>"
    ```

    **Response:**
    ```json
    {
      "access_key": "AKIAIOSFODNN7EXAMPLE",
      "secret_key": "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY",
      "session_token": "AQoDYXdzEPT//////////",
      "endpoint": "https://s3.lyceum.technology",
      "bucket_name": "user-bucket-123",
      "expires_at": "2024-01-15T12:00:00Z"
    }
    ```
  </Tab>

  <Tab title="Python (boto3)" icon="python">
    ```python
    import boto3
    from botocore.config import Config

    # Configure S3 client
    s3_client = boto3.client(
        's3',
        aws_access_key_id='YOUR_ACCESS_KEY',
        aws_secret_access_key='YOUR_SECRET_KEY',
        aws_session_token='YOUR_SESSION_TOKEN',
        endpoint_url='https://s3.lyceum.technology',
        region_name='us-east-1',
        config=Config(signature_version='s3v4')
    )

    # Upload file
    s3_client.upload_file('local.txt', 'user-bucket-123', 'remote.txt')

    # Download file
    s3_client.download_file('user-bucket-123', 'remote.txt', 'local.txt')

    # List objects
    response = s3_client.list_objects_v2(Bucket='user-bucket-123')
    for obj in response.get('Contents', []):
        print(obj['Key'], obj['Size'])
    ```
  </Tab>

  <Tab title="AWS CLI" icon="terminal">
    ```bash
    # Configure credentials
    export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY
    export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY
    export AWS_SESSION_TOKEN=YOUR_SESSION_TOKEN

    # Upload file
    aws s3 cp local.txt s3://user-bucket-123/remote.txt \
      --endpoint-url https://s3.lyceum.technology

    # Download file
    aws s3 cp s3://user-bucket-123/remote.txt local.txt \
      --endpoint-url https://s3.lyceum.technology

    # Sync directory
    aws s3 sync ./local_folder s3://user-bucket-123/remote_folder \
      --endpoint-url https://s3.lyceum.technology
    ```
  </Tab>
</Tabs>

## Bulk Operations

<CardGroup cols={2}>
  <Card title="Bulk Upload" icon="upload">
    Upload multiple files in one request:
    
    ```bash
    curl -X POST https://api.lyceum.technology/api/v2/external/storage/upload-bulk \
      -H "Authorization: Bearer <token>" \
      -F "files=@file1.txt" \
      -F "files=@file2.txt" \
      -F "folder_prefix=data/"
    ```
  </Card>

  <Card title="Bulk Download" icon="download">
    Download multiple files as zip:
    
    ```python
    import requests
    
    response = requests.post(
        'https://api.lyceum.technology/api/v2/external/storage/bulk-download',
        headers={'Authorization': f'Bearer {token}'},
        json={'file_keys': ['file1.txt', 'file2.txt']}
    )
    
    with open('files.zip', 'wb') as f:
        f.write(response.content)
    ```
  </Card>
</CardGroup>

## Best Practices

<AccordionGroup>
  <Accordion title="File Organization" icon="folder-tree">
    Structure your storage for easy navigation:
    
    ```
    /
    ├── datasets/           # Input data files
    │   ├── train.csv
    │   └── test.csv
    ├── models/            # Saved models
    │   └── model_v1.pkl
    ├── scripts/           # Python scripts
    │   └── preprocess.py
    └── outputs/           # Results and outputs
        └── results.json
    ```
  </Accordion>

  <Accordion title="Performance Tips" icon="gauge-high">
    - **Compression**: Use `.gz` or `.zip` for large datasets
    - **Chunking**: Split very large files into smaller chunks
    - **Cleanup**: Remove temporary files after use
    - **Caching**: Reuse files across executions instead of re-uploading
  </Accordion>

  <Accordion title="Security" icon="lock">
    - Never store API keys or credentials in storage
    - Use temporary S3 credentials when sharing access
    - Regularly audit and clean up unused files
    - Keep local backups of critical data
  </Accordion>
</AccordionGroup>

<Note>
  Storage is user-scoped and isolated. You can only access files within your own storage bucket. All data is encrypted and requires authentication.
</Note>

<Warning>
  Avoid storing sensitive information like passwords, API keys, or personal data in cloud storage. Use environment variables or secrets management for credentials.
</Warning>